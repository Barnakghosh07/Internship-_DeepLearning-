{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOtr0VbqbuSTMcNNh0nEDR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8PfcQyVkEvEm","executionInfo":{"status":"ok","timestamp":1686834196587,"user_tz":-330,"elapsed":1296627,"user":{"displayName":"Barnak Ghosh","userId":"12349978207734208546"}},"outputId":"0691b69c-1671-4783-aaa0-d7b272966763"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","Epoch 1/5\n","625/625 [==============================] - 76s 119ms/step - loss: 0.6537 - accuracy: 0.5996 - val_loss: 0.6192 - val_accuracy: 0.6416\n","Epoch 2/5\n","625/625 [==============================] - 73s 116ms/step - loss: 0.5326 - accuracy: 0.7343 - val_loss: 0.5564 - val_accuracy: 0.7172\n","Epoch 3/5\n","625/625 [==============================] - 73s 117ms/step - loss: 0.4626 - accuracy: 0.7825 - val_loss: 0.5820 - val_accuracy: 0.7026\n","Epoch 4/5\n","625/625 [==============================] - 73s 117ms/step - loss: 0.3808 - accuracy: 0.8316 - val_loss: 0.5347 - val_accuracy: 0.7458\n","Epoch 5/5\n","625/625 [==============================] - 73s 117ms/step - loss: 0.3150 - accuracy: 0.8693 - val_loss: 0.7111 - val_accuracy: 0.7504\n","782/782 [==============================] - 19s 25ms/step - loss: 0.6889 - accuracy: 0.7529\n","SimpleRNN Test Loss: 0.6889\n","SimpleRNN Test Accuracy: 0.7529\n","Epoch 1/5\n","625/625 [==============================] - 157s 248ms/step - loss: 0.4094 - accuracy: 0.8144 - val_loss: 0.3275 - val_accuracy: 0.8576\n","Epoch 2/5\n","625/625 [==============================] - 156s 250ms/step - loss: 0.2477 - accuracy: 0.9036 - val_loss: 0.3282 - val_accuracy: 0.8604\n","Epoch 3/5\n","625/625 [==============================] - 153s 245ms/step - loss: 0.1675 - accuracy: 0.9387 - val_loss: 0.3533 - val_accuracy: 0.8480\n","Epoch 4/5\n","625/625 [==============================] - 154s 246ms/step - loss: 0.1490 - accuracy: 0.9438 - val_loss: 0.3925 - val_accuracy: 0.8496\n","Epoch 5/5\n","625/625 [==============================] - 154s 247ms/step - loss: 0.1469 - accuracy: 0.9470 - val_loss: 0.5686 - val_accuracy: 0.7872\n","782/782 [==============================] - 44s 57ms/step - loss: 0.5561 - accuracy: 0.7983\n","LSTM Test Loss: 0.5561\n","LSTM Test Accuracy: 0.7983\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n","\n","# Load the IMDB movie review dataset\n","max_features = 10000  # Maximum number of words to include in the vocabulary\n","max_len = 500  # Maximum length of a review (in terms of number of words)\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# Split the training data into training and validation sets\n","validation_split = 0.2\n","index = int(len(x_train) * (1 - validation_split))\n","x_val, y_val = x_train[index:], y_train[index:]\n","x_train, y_train = x_train[:index], y_train[:index]\n","\n","# Pad sequences to have the same length\n","x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n","x_val = sequence.pad_sequences(x_val, maxlen=max_len)\n","x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n","\n","# Define the SimpleRNN model\n","model_simple_rnn = Sequential()\n","model_simple_rnn.add(Embedding(max_features, 128))\n","model_simple_rnn.add(SimpleRNN(64))\n","model_simple_rnn.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the SimpleRNN model\n","model_simple_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the SimpleRNN model\n","model_simple_rnn.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))\n","\n","# Evaluate the SimpleRNN model on the test set\n","loss, accuracy = model_simple_rnn.evaluate(x_test, y_test)\n","print(f\"SimpleRNN Test Loss: {loss:.4f}\")\n","print(f\"SimpleRNN Test Accuracy: {accuracy:.4f}\")\n","\n","# Define the LSTM model\n","model_lstm = Sequential()\n","model_lstm.add(Embedding(max_features, 128))\n","model_lstm.add(LSTM(64))\n","model_lstm.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the LSTM model\n","model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the LSTM model\n","model_lstm.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))\n","\n","# Evaluate the LSTM model on the test set\n","loss, accuracy = model_lstm.evaluate(x_test, y_test)\n","print(f\"LSTM Test Loss: {loss:.4f}\")\n","print(f\"LSTM Test Accuracy: {accuracy:.4f}\")\n"]}]}